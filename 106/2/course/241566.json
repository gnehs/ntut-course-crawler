[{"name": "\u5c24\u4fe1\u7a0b", "email": "scyou@ntut.edu.tw", "latestUpdate": "2018-02-27 10:28:16", "objective": "\u672c\u8ab2\u7a0b\u4ee5\u6df1\u5165\u6dfa\u51fa\u4e4b\u65b9\u5f0f\uff0c\u4ecb\u7d39\u8207\u6a5f\u5668\u5b78\u7fd2\u4e4b\u76f8\u95dc\u6280\u8853\u53ca\u6f14\u7b97\u6cd5\uff0c\u76ee\u6a19\u662f\u4fee\u7fd2\u672c\u8ab2\u7a0b\u4e4b\u5b78\u751f\u5177\u6709\u95b1\u8b80\u6a5f\u5668\u5b78\u7fd2\u76f8\u95dc\u6587\u737b\u4e4b\u57fa\u672c\u77e5\u8b58\uff0c\u4e26\u80fd\u5f9e\u4e8b\u76f8\u95dc\u4e4b\u5be6\u52d9\u6216\u96fb\u8166\u6a21\u64ec\u5be6\u9a57\u4e4b\u5de5\u4f5c\uff0c\u56e0\u6b64\u672c\u8ab2\u7a0b\u8457\u91cd\u5728\u5ee3\u6cdb\u4ecb\u7d39\u5404\u985e\u4e4b\u6f14\u7b97\u6cd5\u4e4b\u61c9\u7528\uff0c\u800c\u975e\u6df1\u7a76\u5c11\u6578\u6f14\u7b97\u6cd5\u4e4b\u6df1\u5967\u6578\u5b78\u539f\u7406\uff0e\u672c\u8ab2\u7a0b\u6db5\u84cb\u4e4b\u6f14\u7b97\u6cd5\uff0c\u5305\u542b\uff1a\u6700\u4f73\u5316\u57fa\u672c\u77e5\u8b58\uff0c\u96a8\u6a5f\u6700\u4f73\u5316\u6f14\u7b97\u6cd5\uff0c\u96a8\u6a5f\u641c\u5c0b\uff0c\u6c7a\u7b56\u6a39\uff0c\u96a8\u6a5f\u68ee\u6797\uff0c\u985e\u795e\u7d93\u7db2\u8def\u53ca\u6df1\u5ea6\u5b78\u7fd2\u76f8\u95dc\u6f14\u7b97\u6cd5\uff0c\u57fa\u56e0\u6f14\u7b97\u6cd5\uff0c\u8c9d\u6c0f\u5b9a\u7406\u53ca\u61c9\u7528\uff0c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u96b1\u85cf\u5f0f\u99ac\u53ef\u592b\u934a\u53ca\u8a13\u7df4\u6f14\u7b97\u6cd5\uff0c\u652f\u63f4\u5411\u91cf\u6a5f\uff0c\u7dda\u6027\u53ca\u908f\u8f2f\u56de\u6b78\uff0c\u5206\u7fa4\u65b9\u6cd5\uff0c\u964d\u7dad\u65b9\u6cd5\uff0c\u63d0\u5347\u65b9\u6cd5\uff0c\u6c7a\u7b56\u878d\u5408\u6280\u5de7\uff0c\u53ca\u589e\u5f37\u5f0f\u5b78\u7fd2\u7b49\uff0e", "schedule": "Week 1: Class announcement and introduction to machine learning (CH 1)\r\nWeek 2: Basics of spervised learning (Example: K NN), VC dimension, and regression (CH 2). Baysian decision theory (CH 3) and Naive Bayes classifiers (CH 5) \r\nWeek 3: ML and MAP estimation; Bias vs variance dilemma (CH 4). Sample mean mean and sample covariance (CH 5). Dimension reduction techniques: PCA, FA (CH 6)\r\nWeek 4: Dimension reduction techniques: LDA and ICA. Clustering algorithm: k-mean (CH 7)\r\nWeek 5: Unsupervised neural networks (CH 12): Competitive learning, SOFM, and ART. Decision trees: ID3, C4.5, and random forest (CH 8)\r\nWeek 6: Holiday\r\nWeek 7: Decision trees (continued). Mixer model: GMM and EM algorithm (CH 7)\r\nWeek 8: Genetic algorithms. Basics of optiminization.\r\nWeek 9: MT\r\nWeek 10: MT sol. Gradient search and linear discrimination (CH 10)\r\nWeek 11: Feedforward neural networks with examples: radical basis networks and multi-layer perceptrons. Back propagation and regularization methods (CH 11)\r\nWeek 12: Deep learning and convolutional neural networks\r\nWeek 13: More on deep neural networks: Autoencoder, LSTM, and others \r\nWeek 14: SVM (CH 13) and HMM (CH 15)\r\nWeek 15: Combining multiple classifiers (CH 17)\r\nWeek 16: Design and analysis of experiments (CH 19)\r\nWeek 17: Reinforcement learning (CH 18)\r\nWeek 18: Final exam", "scorePolicy": "MT 30 %\r\nFinal 40 %\r\nHW 30 %\r\nProject 10 % (optional)", "materials": "Reference text book: Introduction to machine learning. E. Alpaydin. 2nd ed or 3rd ed. Note: textbook is only used to follow the presentation order. Much of the detailed lecture materials are NOT covered in the textbook.", "foreignLanguageTextbooks": true}]